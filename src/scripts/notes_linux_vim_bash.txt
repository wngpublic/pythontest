* toc_linux
* toc_cmdline
* toc_java
* toc_python
* toc_systems
* toc_linux
* toc_intellij_shortcuts
* toc_sql
* toc_redis
* toc_end
--------------------------------------------------------------------------------
* toc_cmdline
- toc_cmdline_tcpdump
- toc_cmdline_ssh
- toc_cmdline_java
- toc_cmdline_memcache
- toc_cmdline_vim
- toc_cmdline_curl
- toc_cmdline_linux
- toc_cmdline_jq
- toc_cmdline_awk
- toc_cmdline_bash
* toc_cmdline_end


- toc_cmdline_tcpdump
  tcpdump -i eth0
  tcpdump -A -i eth0                  // get eth0 and print in ascii
  tcpdump -XX -i eth0                 // get eth0 and print in hex and ascii
  tcpdump -w file.pcap                // capture and save to pcap
  tcpdump -r file.pcap                // read from pcap
  tcpdump -tttt -r file.pcap          // read from pcap very verbose
  tcpdump -n                          // include ip addresses (default is dns)
  tcpdump -c 1000 tcp                 // capture only 1000 tcp packets
  tcpdump tcp                         // include only tcp packets
  tcpdump port 8000                   // capture from only port 8000
  tcpdump src ipaddr                  // capture only from src == ipaddr
  tcpdump dst ipaddr                  // capture only from dst == ipaddr
  tcpdump dst ipaddr and port 8000    // capture only from dst -> port 8000
  tcpdump -D                          // show available interfaces

  options
    -A                                print each packet in ASCII
    -c <N>                            after receiving N packets, exit
    -C <N>                            N file size, then open new file, need file output
    -d                                dump human readable
    -e                                print link level header on each dump line
    -F <file>                         input file as filter expression
    -i <ifc>                          listen on interface
    -l                                lower case l, stdout line buffered
    -n                                do not convert IP address to name
    -nn                               do not convert IP and PORT
    -N                                do not print domain name qualification of host names
    -r <file>                         input file read packets from file
    -s <N>                            snaplength/capture size of packet, default is 68 bytes, then throw away
    -s 0                              save all contents of packet, do not discard
    -t                                do not print timestamp on each dump line
    -tttt                             print timestamp as h:m:s:fraction of second
    -v,-vv,-vvv                       verbose
    -w <file>                         write to file
    -x                                print data of each packet in hex
    -X,-XX                            print data of each packet in hex and ASCII
    -D                                view which interface available for capture, eg tcpdump -D

  filters
    src <ip>
    dst <ip>
    port <port>
    host <ip>                         filter traffic to AND from specific host, host can be src or dst
    icmp                              icmp packets only
    udp                               udp packets only
    tcp                               tcp packets only
    <=                                less than,equal packet size, eg tcpdump <= 128
    less|greater                      tcpdump less 128, tcpdump greater 128   packet size

  packet metadata filter
    or,and,not,(...),=,!=,||,&&,!

  output explanation
    Flags
      S                               SYN connection
      F                               FIN connection
      P                               PUSH data
      R                               RST connection
      .                               ACK

    <timestamp hr:min:s:milli> IP <src_ip.port>   > <dst_ip.port>:     Flags[SFPR.], seq <byte_start:byte_end>  <ack_num>   <window_size>   <tcp_options>       <packet_length_in_bytes>
    xx:xx:xx.xxxxx             IP xx.xx.xx.xx.xxx > xx.xx.xx.xx.xxx    [P.]          seq 123:456                ack 1       win 222,        options [...],      length 555


    <timestamp hr:min:s:milli>        timestamp
    IP                                type of packet,can also by ARP
    <src_ip.port>                     src ip and port
    >                                 direction
    <dst_ip.port>:                    dst ip and port
    Flags[SFPR.],                     flags [SFPR.]
    cksum                             checksum hex
    seq <byte_start:byte_end>         byte_end-byte_start == packet_length in bytes
    <ack_num>
    <window_size>                     src host TCP window
    <tcp_options>                     check TCP field definitions
      TOS                             type of service
      proto                           TCP
    <packet_length_in_bytes>




  examples:
    tcpdump -i eth0 -l -c 1000        always try to do buffered -l
    tcpdump -i eth0
    tcpdump -i eth0 -c 100
                                      100 packets
    tcpdump -i eth0 -c100 -nn
                                      100 packets, do not convert IP address and port
    tcpdump -i eth0 -c5 icmp          5 packets of ICMP packets
    tcpdump -i eth0 -c5 -nn host 123.123.123.123
                                      5 packets to host 123.123.123.123, do not convert IP or port
    tcpdump -i eth0 -c5 -nn port 80
                                      5 packets to and from port 80
    tcpdump -i eth0 -c10 -nn src 1.1.1.1
                                      10 packets from host 1.1.1.1
    tcpdump -i eth0 -c10 -nn dst 2.2.2.2
                                      10 packets to host 2.2.2.2
    tcpdump -i eth0 -c10 -nn src 1.1.1.1 dst 2.2.2.2
                                      10 packets from host 1.1.1.1 to host 2.2.2.2
    tcpdump -i eth0 -c10 -nn src 1.1.1.1 and port 80
                                      10 packets from host 1.1.1.1 to port 80
    tcpdump -i eth0 -c10 -nn "(src 1.1.1.1 and port 123)" "(dst 2.2.2.2 and port 80)"
                                      10 packets from host 1.1.1.1:123 to host 2.2.2.2:80
    tcpdump -i eth0 -c10 -nn "(src 1.1.1.1 and dst 2.2.2.2 and port 80)"
    tcpdump -i eth0 -c10 -nn -w filename port 80
    tcpdump -nn -r filename
    tcpdump -nn -r filename src 1.1.1.1
                                      read from filename with src 1.1.1.1
    tcpdump -i eth0 -c10 -nn host 1.1.1.1
                                      10 packets to and from host 1.1.1.1 (src == host || dst == host)
    tcpdump -i eth0 -c10 -nn src host 1.1.1.1
                                      10 packets to and from host src 1.1.1.1
    tcpdump -i eth0 -c10 port 80 && port 90
                                      10 packets with ports 80 AND 90 (doesn't matter if src or dst)
    tcpdump -i eth0 -c10 'port 80 && port 90'
                                      10 packets with ports 80 AND 90 (doesn't matter if src or dst)
    tcpdump -i eth0 -c10 'port 80 || port 90'
                                      10 packets with ports 80 OR 90 (doesn't matter if src or dst)
    tcpdump -i eth0 '(port 80 or port 90) and host 1.1.1.1'
    tcpdump -i eth0 not port 80
    tcpdump -nnvvS                    very verbose
    tcpdump -nnvvXSs 10000            snaplength
    tcpdump 'tcp[13] & 16 != 0'       show all ACK packets
    tcpdump 'tcp[13] & 32 != 0'       show all URG packets
    tcpdump 'tcp[13] & 2 != 0'        show all SYN packets
    tcpdump 'tcp[tcpflags] & (tcp-syn|tcp-ack) == (tcp-syn|tcp-ack)'
                                      show TCP SYN/ACK
    tcpdump -i eth0 -s0               snap length 0, needed for pulling binaries/files
                                      if no -s0, then snap length is 68 bytes!
    tcpdump -nn -A -s1500 -l | grep "User-Agent:"
                                      snap length 1500 and line buffered ASCII, so can grep
    tcpdump -s 0 -A -vv 'tcp[((tcp[12:1] & 0xf0) >> 2):4] = 0x47455420'
                                      capture only HTTP get and POST packets
                                      tcp[((tcp[12:1]&0xf0)>>2):4] first determines location
                                      of bytes and then selects the 4 bytes to match against
    tcpdump -nn -A -s 0 -l | egrep -i 'Set-Cookie|Host:|Cookie:'
                                      capture cookies from server from client
    tcpdump ssh                       check all packets used on ssh
    tcpdump net 192.168.1.0/24        get packets for entire subnet
    tcpdump portrange 22-25
    tcpdump -i eth0 src port not 22
    tcpdump -c 2000 -s 0 -nn -l -w tmp.tcpdump.1.pcap


  ssldump -k /path/key_file.key -i eth0 -c10 port 80 src 1.1.1.1

  ssldump can only decrypt SSL/TLS packet if RSA keys are used. if DHE or RSA ephemera
  cipher suite used, RSA keys used only to secure DH or RSA exchange, but not encrypt.

- toc_cmdline_ssh
    - options
        -N      do not open shell or execute program on remote side
        -D      dynamic port forwarding/proxy using SOCKS
        -L      local port forwarding
        -R      remote port forwarding
        -f      causes SSH to go to background before executing
        -v      debug info
        -C      compression enable
    /etc/ssh/sshd_config

    - dynamic port forwarding/proxy
    ssh -D local_host:local_port user@ssh_server
    ssh -D local_port user@ssh_server

    ANYWHERE <-> SSH_SERVER <-> LOCAL_HOST:LOCAL_PORT <->

    - remote port forwarding
    ssh -R remote_port:target_host:target_port user@ssh_server
    connect to target_host:target_port on local host A, and remote_host:remote_port accesses its own
    127.0.0.1:remote_port, which then connects to local host A to connect to target_host:target_port
    remote_host types 127.0.0.1:REMOTE_PORT

    TARGET_HOST:TARGET_PORT <-> LOCAL_HOST <-> SSH_SERVER:22 <-> SSH_SERVER:REMOTE_PORT

    ssh -R remote_port:local_host:local_port user@ssh_server
    TARGET_HOST:TARGET_PORT <-> LOCAL_HOST <-> SSH_SERVER:22 <-> SSH_SERVER:REMOTE_PORT


    - local port forwarding
    ssh -L local_port:target_host:target_port user@ssh_server
    connect to target_host:target_port on local_host:local_port. so on local_host, access
    127.0.0.1:local_port, which then connects to target_host:target_port
    local_host types 127.0.0.1:LOCAL_PORT

    TARGET_HOST:TARGET_PORT <-> SSH_SERVER:22 <-> LOCAL_HOST:LOCAL_PORT

    - local port forwarding where target_host is ssh_server
    ssh -L local_port:localhost:target_port user@ssh_server
    ssh -L local_port:127.0.0.1:target_port user@ssh_server
    LOCAL_HOST:TARGET_PORT <-> SSH_SERVER:22 <-> LOCAL_HOST:LOCAL_PORT


    - examples
    ssh remote_host 'tcpdump -c 100 -A' | wireshark
        capture tcpdump from remote host and pass to local wireshark
    tar -cvf /folder_local | ssh remote_host 'tar -xvf /folder_remote'
        compress local folder and send to remote_host to uncompress to folder_remote
    ssh remote_host 'tar -cvf - /folder' > local.tar.gz
        copy remote folder to local archive
    ssh user@remote_host command
        run command on remote_host

- toc_cmdline_java
-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005

// -agentlib must be before classname and args
java -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005 -Xdebug Main console

java -jar build/libs/service.jar
jar tvf jarfile
jstack -l <pid>
jmap -dump:format=b,file=heapdump.hprof <pid>
jmc                                   // java mission control collects and analyzes
                                      // it is gui to record stuff
jcmd                                  // wrapper for jstack, jmap, etc

oql examples:
SELECT s FROM java.lang.String s WHERE s.toString().contains("stilmzhjhvon")
select * from "com.google.common.cache.*"



- toc_cmdline_memcache

memcache operations
  start
    memcached -d -p <port>
  flush
    echo flush_all | nc localhost <port>
  dump all keys
    echo "stats items" | nc localhost <port> | grep "number "
      this returns a set of populated slabs, eg
      STAT items:7:number 6
      STAT items:11:number 1
    echo "stats cachedump 11 0" | nc localhost <port>
      this returns a set of key values, one item in this case
      ITEM key value

memcache commands:
  telnet 127.0.0.1 <portnum>
  stats items
  stats cachedump <slabnum> <limit>
    stats cachedump 22 100
  stats slabs     // current mem stats
  stats sizes
  stats
  flush_all       // invalidates all existing cache items
  set <key> <flags> <ttl_seconds> <size_bytes>      // ensure to use \r\n for line breaks
      In the following example, we use tutorialspoint as the key and set value Memcached in it with an expiration time of 900 seconds.
      set tutorialspoint 0 900 9
      memcached                       // this size is 9, so you need to know size!!
      STORED
      get tutorialspoint
      VALUE tutorialspoint 0 9
      Memcached
    set key1 0 10 5 data1
    set keyNg1 0 10 6 hello1
    set keyNg1 0 10 6 \r\nhello1\r\n    // new line, hello1, then newline. so layout looks like:
      set keyNg1 0 10 6
      hello1
      STORED            // status
    set k999 0 30 6
    hello2
    STORED
    then do stats items, which shows a slab has 1 extra item.
    stats cachedump <slabnum> <limit>
    get key
  add <newkey> <flags> <ttl> <size>
  replace <key> <flags> <ttl> <size>
  get <key>
  delete <key>
  touch <key> <ttl>       // update ttl without fetching key
  dump all keys
  stats cachedump <slab> <limit>      // if limit == 0, then all keys in slab
  quit


MEMCHOST=localhost; printf "stats items\n" | nc $MEMCHOST 11211 | \
  grep ":number" | awk -F":" '{print $2}' | \
  xargs -I % printf "stats cachedump % 0\r\n" | nc $MEMCHOST 11211

MEMCHOST=localhost; printf "stats items\n" | nc $MEMCHOST 11211 | \
  grep ":number" | awk -F":" '{print $2}' | \
  xargs -I % printf "stats cachedump % 0\r\n" | \
  nc $MEMCHOST 11211 | grep ITEM | awk '{print $2}' | \
  sed -e 's/"/\\"/g'| xargs -I % printf "get %\r\n" | nc $MEMCHOST 11211



* toc_cmdline_misc
mvn package
mvn install
mvn clean

- toc_cmdline_vim


:<range>s/pat/rep/cgiI    c   confirm each sub
                          g   all occurrence
                          i   ignore case for pattern
                          I   don't ignore case for pattern

range arg:
  %                           whole file
  .
  number                      absolute line of number

%s:/a/b/c/d:/a/b/c/e:g        use : instead of / to avoid backslash

vim regex group substitution
%s!slave \(\w\+\) 0 \(\d\+\) \(\d\+\) connected!slave \1 0 \3 connected!gc

%s!\(\w\+\)!\1!g                # (abc)   -> abc    \( is grouping
%s/(fixed word (\([0-9]\+\), \([A-Z\_0-9"]\+\),\([a-zA-Z\_0-9"]\+\)))/ \2 \3/gc
    // eg i: (blah blah (fixed word 123, "ABC_123","CDE_234")))
    //    o: (blah blah "ABC_123" "CDE_234")
%s!([a-zA-Z0-9]\+)!()!g         # (abc)   -> ()     ( is literal
%s!([a-zA-Z0-9./_]\+)!()!g      # (a/b._c)   -> ()     ( is literal
%s!(\[[a-zA-Z0-9./_]\]\+)!()!g
    # ([a/b._c])   -> ()     ( is literal
%s!"\([a-zA-Z]\+\)":\[["0-9,]\+\]!"\1":\[\]!gc
    # "someword":["123","234"]   -> "someword":[]
%s! \d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3} ! IP !g
    # wrong
%s! \d\{1,3\}\.\d\{1,3\}\.\d\{1,3\}\.\d\{1,3\} ! IP !g
    # right!
%s!["0-9,]\+!\[\]!g
    # "123","234" -> []    no literal bracket
%s!\[["0-9,]\+\]!\[\]!g
    # ["123","234"] -> []  has literal bracket

regex
  \s \S
  \d \D
  \x \X
  .
  \w \W
  \a \A

quantifiers greedy
  *
  \+
  \=
  \{n,m}
  \{n}
  \{,m}
  \{n,}

quantifiers non greedy
  \{-}                        0 or more of preceding, as few as possible
  \{-n,m}
  \{-n,}
  \{-,m}

[characterrange]

swapping and grouping

  s:\(\w\+\)\(\s\+\)\(\w\+\):\3\2\1:    swap group 1 2 3 with 3 2 1
        1       2       3

global command, search and execute

  :range g/pat/cmd                      where match occur, do cmd
  :range g!/pat/cmd                     where match not occur, do cmd

  :g/^$/ d                              delete all empty lines
  :g/^






:registers          view all registers
:reg a              view register at a
:let @l='....'      edit a register
q<key>seq cmd<esc>q
n|                  jump to column, eg 300| to column 300
gt                  go to next tab
:g/pattern/d        delete all lines matching pattern
:g!/pattern/del     delete all lines not matching pattern

^M                  0xD 0xA newline. To remove, :%s/^M//g -> :%s/CTRL v + CTRL m//g
                    this also works             :%s/\r//g



* toc_cmdline_curl

curl -x               proxy, seems like same as default
-v                    verbose
-u                    username
--trace <file>        full trace dump
--trace-ascii <file>
-p                    proxy tunnel
-L                    follow redirect, eg curl -vL URL
--anyauth             automatically use most safe method, so do unauth first and switch to auth if needed

curl -o file_local url/filename
    download file with rename file
curl -O url/filename
    download file
curl -OC - url/filename
    download file continue
wget url/filename
    same as curl -O url/filename
wget url/filename1 url/filename2
    download multiple files
wget -c url/filename
    resume download
curl -L url
    follow redirects
curl url | python -m json.tool
curl url | jq '.'
curl -v -H '<header>' -H '<header>' -d '{json}' '<url>'    # -X POST is implicit with -d
curl -v -H '<header>' -H '<header>' -d @filejson '<url>'
curl -k '<https_insecure_url_to_disable_certificate_checks>'
curl -F [email]
curl -K <configfile> <url>
curl -u <name>:<password> <url>
curl -u <name> <url>            # with password prompt
curl -L -b non-existing <url>   # cookie but empty internal cookie to start off with
curl -L -b <cookiefile>         # read from cookie, no updates in response
curl -L -b <cookiefile> -c <cookiefile> # read and write cookie, update from response
curl -j -b <cookiefile> -c <cookiefile> <url>   # new cookie session
curl -d '{json}' -H 'content-type: application/json' <url>
curl -d @jsonfile -H 'content-type: application/json' <url>
curl -d 'k1=v1&k2=v2' <url>
curl -data-binary @filename <url>



python3 -m json.tool tmp.json
cat xmlfile | xmllint --format - > prettyxml.txt
xmllint --format inputxml.txt
pretty: python3 -m json.tool tmp.json

- toc_cmdline_linux

netstat -a          // active tcp
netstat -r          // routing table
netstat -i
netstat -o          // display PID for each connection
netstat -n          // do not display hostname
netstat -f          // address family
netstat -c          // monitor continuously
netstat -s          // stats
netstat -t          // show TCP
netstat -v          // increase verbosity
netstat -x          // show listeners
netstat -e          // extra info, stats
netstat -p          // display pid of prgram using socket
netstat -l          // listening connections only
netstat -W          // do not truncate hostnames on mac, sometimes it's -T on linux
        -T          // linux equivalent, eg netstat -aT     no trim, longform
netstat -aT
netstat -ant | grep 8080 | grep EST | wc -l

netstat -atvl | grep EST

netstat -atvlW | grep EST

netstat -atvlT | grep EST


ifconfig

find . -name "tmp*" | xargs rm -rf
find . -name "*.tmp" | xargs rm
find . -name "*.tmp" -exec rm {} \;   is equivalent to above
find . -name "*.tmp" | xargs -p rm          # prompt each xargs cmd
find . -name "*.tmp" | xargs -t rm          # print out cmd executed
find . -name "*.tmp" | xargs -0 rm          # deal with space in filenames
ls "file*" | xargs wc                       # ls files and pass to wc for each file


sed -E 's/[0-9]+ AbC [0-9]+ [0-9:,]+ \[STRING\]  \(thread-[0-9]+\) a.b.c.classname: //g' xak > xak.sedout
sed -E 's!^[0-9\.]+ \[[0-9]+ [0-9\.:]+\] !!g' snip.txt > snip.sed.txt
  eg        ^123.123 [123 123.123:123] blah blah
            ^blah blah

rsync is secure, with ssh allowable
rsync options src dst
rsync -zvr srcdir dstdir
    z                                       compression
    r                                       recursive
    v                                       verbose
    a                                       archive
    u                                       update, skip files newer on dst
    l                                       copy symlinks
    b                                       make backup
    n                                       dry run
    -e ssh                                  use ssh
rsync -avz -e ssh srcdir user@host:dstdir
    rsync to remote
rsync -avz -e ssh user@host:srcdir dstdir
    rsync to local



sort <file> | uniq -c | sort
uniq -c <file> | sort           # file has to be sorted first, based on experiment. the sort is for count

split
split -l 200000 filename        // split after 200k lines
split -C 5m --numeric-suffixes filename
vmstat      // memory stats
free        // free mem

nslookup <ipaddr>
dig -x <ipaddr>
host <hostname>
host -t cname <hostname>

- toc_cmdline_jq


cat <jsonfile> | jq             // prints out in json format
cat <jsonfile> | jq '.<key>     // prints the value of key(s)
cat <jsonfile> | jq '.<key1>.<key2>.key3[0].key4'   // get all match this hiearchy
always wrap jq in single quotes, else bash tries to interpret all symbols like .,
  whereas we want jq to do that
iteration: jq '.[]'             // output each array value in separate lines
jq '.[].key'                    // output each array key in separate line
jq '.[0].key'
jq '[10:15]'                    // slice

bash -x syntax.sh

debugging on part of script, set -x to active, set +x to stop debug

- toc_cmdline_awk


awk '{ if($3 ~ /regex/){ d[$4]=$1; } else if($1 in d){ print($1 " exists"); } else { d[$1]=""; } }'

awk '{ if($3 ~ /regex/){ if($4 in d && d[$4] != ""){ print($4 " in dict " d[$4]); } else { d[$4]=$1; } } else if($1 in d == 0){ d[$1]=""; } } END { print("--------"); for(k in d){ print(k " " d[k]); } }'

awk '{ if($3 ~ /regex/){ if($4 in d && d[$4] != ""){ print($1 " is duplicate " $4 " " d[$4]); } else { d[$4]=$1; } } else if($1 in d == 0){ d[$1]=""; } } END { print("--------"); for(k in d){ if(d[k] == ""){ print(k " " d[k]); } } }'

awk '{ if($3 ~ /regex/){ if($4 in d && d[$4] != ""){ print($1 " is duplicate " $4 " " d[$4]); } else { d[$4]=$1; } } else if($1 in d == 0){ d[$1]=""; } } END { print("-------- nodes "); for(k in d){ if(d[k] == ""){ print(k " " d[k]); } } }'

awk '{ if($3 = "abc"){ if($4 in d && d[$4] != ""){ print($1 " is duplicate " $4 " " d[$4]); } else { d[$4]=$1; } } else if($1 in d == 0){ d[$1]=""; } } END { print("-------- nodes "); for(k in d){ if(d[k] == ""){ print(k " " d[k]); } } }'

// counts number of times
awk '
BEGIN { print("get \"li\"") }
/li/ { n++ }
END { print("\"li\" appears ",n," times") }' fileinputname

// prints first three tokens of each record
awk '
{
    for (i = 1; i <= 3; i++)
        print $i
}' inventory-shipped


{
    if ($1 > max)
        max = $1
    arr[$1] = $0
}

END {
    for (x = 1; x <= max; x++)
        print arr[x]
}

cat myFile | while read x ; do echo $x ; done
cat myFile | while read x y ; do echo $y $x ; done    # auto split words
while read x y ; do echo $y $x ; done < myFile

while read i; do echo $i; done < <(echo "$FILECONTENT")

echo $FILECONTENT |
{
while read i; do echo $i; done
...do other things using $i here...
}

while read i; do echo "$i"; done <<<"$FILECONTENT"


echo "hello" >&1      # redirect stdout to stdout
echo "hello" >&2      @ redirect stdout to stderr
echo "no changes" >&1 | sed "s!no!some!"      # changes go through pipe because stdout
echo "no changes" >&2 | sed "s!no!some!"      # changes dont go through because stderr
echo "hello" >> file                          # append
echo "hello" > file                           # out
cmd 2>&1
cmd > file 2>&1
cmd 2> /dev/null                              # redirect stderr to null
cmd 2> file                                   # redirect stderr to file
cmd 1>&2 file                                 # stdout to same filedescriptor as stderr
cmd 2>&1 file                                 # stderr to same filedescriptor as stdout
cmd 1> /dev/null                              # redirect stdout to null
cmd  > /dev/null                              # redirect stdout to null
cmd &> /dev/null                              # redirect stdout and stderr to null
cmd >  /dev/null 2>&1 &                       # redirect stdout and stderr to null background
cmd >  /dev/null 2>&1                         # redirect stdout and stderr to null
cmd >  file 2>&1                              # redirect stdout and stderr to file

{
in bash, if redirect stderr and stdout within script, add this to beginning:
#!/bin/bash
exec &> log.txt
echo "piping to log.txt"
other code

which is the same as:
cmd &> log.txt

# inside script:
# send stdout to file
exec > log.txt

# send stdout and stderr to file
exec > log.txt
exec 2>&1

# append stdout and stderr to file
exec >> log.txt
exec 2&1

}

- toc_cmdline_bash

internal variables
$0                              name of shell script
$!                              process ID of job most recently asynchronously executed
$$                              process ID
$*                              position params. joined $1c$2c... where c is first char of value of IFS var
$@                              position params. space delimited $1 $2 ...
$BASH_ARGC                      $#
$BASH_ARGV                      ${!#}
$BASH_COMMAND
$BASH_EXECUTION_STRING
$BASH_LINENO
$BASH_SOURCE

{
the read command is provided by the shell for reading text from standard input into a shell variable


--------------------------

#!/bin/bash

read foo
echo "You entered '$foo'"

$ echo bob | myscript.sh
You entered 'bob'

--------------------------

echo "hello world" | { read test; echo test=$test; }
read_from_pipe() { read "$@" <&0; }
cat myFile | while read x ; do echo $x ; done
cat myFile | while read x y ; do echo $y $x ; done

--------------------------

read test < <(echo hello world)
echo $test

--------------------------

#!/bin/sh
cat -

echo test | sh my_script.sh

--------------------------

#!/bin/bash
while IFS= read -r line; do
  printf '%s\n' "$line"
done


--------------------------

$ cat reader.sh
#!/bin/bash
while read line; do
  echo "reading: ${line}"
done < /dev/stdin

$ cat writer.sh
#!/bin/bash
for i in {0..5}; do
  echo "line ${i}"
done

./writer.sh | ./reader.sh

--------------------------

AWK

BEGIN {
    a[1] = 1
    a[2][1] = 21
    a[2][2] = 22
    a[3] = 3
    a[4][1][1] = 411
    a[4][2] = 42

    process_array(a, "a", "do_print", 0)
}

function do_print(name, element)
{
    printf "%s = %s\n", name, element
}

----------------------------------

function walk_array(arr, name,      i)
{
    for (i in arr) {
        if (isarray(arr[i]))
            walk_array(arr[i], (name "[" i "]"))
        else
            printf("%s[%s] = %s\n", name, i, arr[i])
    }
}

BEGIN {
    a[1] = 1
    a[2][1] = 21
    a[2][2] = 22
    a[3] = 3
    a[4][1][1] = 411
    a[4][2] = 42

    walk_array(a, "a")
}

awk -f walk_array.awk
-| a[1] = 1
-| a[2][1] = 21
-| a[2][2] = 22
-| a[3] = 3
-| a[4][1][1] = 411
-| a[4][2] = 42


function process_array(arr, name, process, do_arrays,   i, new_name)
{
    for (i in arr) {
        new_name = (name "[" i "]")
        if (isarray(arr[i])) {
            if (do_arrays)
                @process(new_name, arr[i])
            process_array(arr[i], new_name, process, do_arrays)
        } else
            @process(new_name, arr[i])
    }
}

for (i = 1; i <= 100; i *= 2)
    print i



awk -f program

}

ls -l | awk '$6 == "Nov" { sum += $5 }
             END { print sum }'


- toc_cmdline_sed

- toc_cmdline_grep

egrep == grep -E                  # so you don't need backspace for special regex characters
grep "t[wo]o" file                # two | too
grep "..bt" file                  # xxbt
grep "[^c]ode" file               # NOT match code but any other char
grep "^[a-z]" file                # every beginning alphachar
grep "[[:upper:]]" file           # every beginning uppercase alpha
grep "([A-Z]*)" file              # match 0 or more times

[:alnum:]
[:alpha:]
[:blank:]                         # space or tab
[:digit:]
.                                 # any single char
?                                 # preceding item is optional and matched at most once
                                  # basically 0 or 1 time
*                                 # preceding item matched 0 or more
+                                 # 1 or more
{N}                               # exactly N times
{N,}                              # N or more times
{N,M}                             # at least N but not more than M times



grep "[[:digit:]]" file
grep "[[:digit:]][[:alpha:]]" file    # digit followed by alpha
grep "[ ]\+hi"                        # multi space followed by hi
grep "[ ]\?hi"                        # only 0 or 1
grep "[ ]+"                           # space followed by +
grep "^[^ ]$" file                      # no space
grep -E "^[ ]{10}$" file
grep "^[ ]\+$" file
grep "^[0-9]\+$" file                 # only numbers
grep "[ ][0-9]\{2\}$" tmp.txt         # space and exactly 2 numbers
grep ".[0-9]\{2\}$" tmp.txt           # any char and exactly 2 numbers
grep -E ".[0-9]{2}$" tmp.txt          # any char and exactly 2 numbers. -E removes the backslash
grep "\<anchor" file                  # anchor to beginning of word
grep "anchor\>" file                  # anchor to back of word
grep "\<anchor\>" file                # anchor to beginning and back
grep -E '\breg' file                  # \b is word boundary, if you use -e
grep -E 'hi|bye' file
grep -E '^[a-z]+|[0-9]+$' file        # -E seems to do regex without backslash
grep "$HOME" file                     # grep the variable
grep '$HOME' file                     # grep literal $HOME
grep `whoami` file
grep "^[a-z].*\.$" file               # any characters followed by literal .
grep -E "(A|B|C)" file                # grep A or B or C
grep "hi..hi" file                    # grep hi then any 2 char then hi
grep 'hi..hi' file                    # grep hi then any 2 char then hi
grep -E "hi..hi" file                 # same
grep -E 'hi[.a-z]{2}hi' tmp.txt       # period or a-z
grep 'hi\.\.hi' file                  # grep hi then .. then hi
grep -E '^[^2]{4}$' tmp.txt           # match any word that is not 2 four times
egrep "^[0-9]*$" file                 # match only numbers and blank space, * is for 0 or more
egrep == grep -E                      # just use grep instead of grep -E
fgrep == grep -F

find <dir> -type f --min +120 | xargs rm -f     # find all files older than 120 min and pass to remove




- toc_cmdline_nc

nc can create almost any type of connection: tcp proxy, shell script http
clients and servers, network daemon testing, socks, http proxycommand for ssh.

nc  -46bCDdhklnrStUuvZz -I length -i interval -O length
    -P proxy_username -p port -q seconds -s source -T toskeyword
    -V rtable -w timeout -X proxy_protocol -x proxy_address destination port

    -D debug
    -C CRLF
    -d do not read from stdin
    -h help
    -I size of tcp receive buffer
    -i interval between lines of text sent and received
    -n do not DNS
    -l listen for incoming connection, cannot be used with -p, -s, -z
    -p source port
    -s source IP
    -u udp
    -V use specified routing table
    -v verbose
    -w timeout
    -X proxy_protocol
    -x proxy_address:port
    -z specifies to scan for listening daemons, without sending data to them

nc -l 1234                        # listen port 1234
nc 127.0.0.1 1234                 # connect on port 1234
nc -l 1234 > out                  # output captured
nc hostname 1234 < input          # feed into host
nc -zc host 20-30                 # port scan
nc -l 1234 | tar xvzf -           # incoming file will be unzipped and extracted
tar -czf - * | nc host 1234       # zip and pipe to 1234


nc [options] host port

nc is similar to telnet command

netcat is good for tcp and udp, but can also listen on port for connections and
packets. this allows 2 instances of nc to establish client server


iostat              reports cpu, disk io, nfs
tcpdump
netstat             read network info, routing tables, ifc, network connections
bzip2 file          compress
bzip2 -d file.bz2   decompress    bz2
shutdown -h now
crontab -u user -l      cron jon schedule and see existing
ifconfig
uname               host kernel stuff
locate              locate filenameregex
mysql -u username -p -h hostip
mysql -u root -p
yum                 install
rpm                 install
wget url_file       download file




- toc_cmdline_end

--------------------------------------------------------------------------------
* toc_git

commonly used git
  git config --list
  git config --local user.name "abc"
  git config --local user.email "abc"
  git push --force branchname
  git add -i
  git branch --list
  git branch --delete <oldbranch>
  git branch <newbranch>
  git branch -m <oldname> <newname>
  git branch -m <newname>       // if on branch to change
  git push origin --delete feature/branch  // delete remote branch
  git branch -d feature/branch             // delete local branch
  git push origin --d branchname           // if in the current package
  git checkout <branch|remote-branch>
  git checkout -b newbranch     // create branch and checkout
  git push origin branchname
  git push --set-upstream origin <local-branchname>
  git push origin --delete <remote-branchname>
  git fetch origin pull/ID/head:BRANCHNAME
  git checkout BRANCHNAME
  git fetch origin              // not applied, only fetched
  git pull
  git commit -m "message"
  git commit --amend -m "newmessage"
  git commit --amend            // change the message of git log
    // goes to commit file, write update message and quit
  git commit --amend --no-edit
  git diff > <patch>
  git apply <patch>
  git cherry-pick <sha1_of_commit_from_git_log>
  git checkout -b <newbranch> <sha1_of_mainline>
  git format-patch -1 <sha1_of_commit>
    patch -p1 < file.patch      // not git patch, just patch
  git log origin\master
  git log --grep='commit message'
  git log --author=authorname
    git show <commitid>           // use git log to get commitid
  git log HEAD
  git log -n 3                  // show last 3 logs
  git log --oneline
  git log --name-only -n 3      // filenames of last 3 commits
  git log --name-only           // file-name file-only
  git log -p -n 3               // show patches of last 3
  git log -- filename           // show log of filename for past 3
  git log -n 3 -- filename      // show log of filename for past 3,
                                // -n 3 must be before
  git show <commitid>           // use git log to get commitid
  git show hashid:/path/file
  git rebase
  git rebase --continue
  git rebase -i HEAD~1          // change the git commit message
                                // :r for reword > goes to next page
                                // change summary and save
                                // push it to whereever
  git remote add origin https://github.com/blah   // add remote as origin
  git push -u origin master                       // push to origin
  git reset --hard
  git reset --soft hashid
  git stash list
  git stash show stash@{2}
  git stash show -p stash@{2}   // full details
  git stash show -p stash@{2} > patchfile     // patch file
  git apply --stat patchfile
  git apply --check patchfile
  git apply patchfile
  git format-patch -<n> <SHA1> --stdout > patchfile
  git format-patch -1 SHA1 --stdout > patchfile   // -stdout is REQUIRED
  git format-patch -10 SHA1 --stdout > patchfile
  git stash apply stash@{1}
  git stash save "message" -u   // stash including untracked
  git stash drop stash@{2}
  git stash clear
  git show hashid:/path/file
  git show hashid               // shows all files of that hashid
  git checkout hashid -- path/file      // revert file to that hashid
  git reset hashid path/file
  git branch -m new_brancname   // rename branchname of current branch
  git branch -m oldname newname // rename oldbranch to newbranch if on different branch
  git branch -a                 // list existing branches from remote
  git checkout <remotebranch>   // checkout remote branch, DO NOT DO git checkout -b <remotebranch>
  git push origin :oldname newname    // delete oldname remote branch and push the newname local branch
  git push origin -u newname          // reset upstream branch for newname localbranch
  git branch                          // list branches
  git subtree
  git tag -l
  git checkout tags/v2.0.5
  git checkout tags/v1.9.3 -b version-1.9.3
  git merge --squash branchname
  git merge --squash branchname     // merge branchname commits into master and squash all commit messages into one commit
    git checkout master
    git merge --squash branchname
    git commit
                                    // alternative to --squash is git rebase -i and rewrite commit msg
    equivalent to:
      git checkout branchname
      git diff master > feature.patch
      git checkout master
      patch -p1 < feature.patch
      git add .
                                    // alternative is git reset --soft and the rewrite commit log
// port file from another repository into different repository, preserving the git history of the file
cd repository
git log --pretty=email --patch-with-stat --reverse --full-index --binary -- path/to/file_or_folder > patch
cd ../another_repository
git am < ../repository/patch        // git apply with commit. use this to preserve the git history. then use git mv.
git apply < ../repository/patch     // git apply without commit. if using this, the git history gets lost

Extract history in email format using
git log --pretty=email -p --reverse --full-index --binary

git checkout otherbranch myfile.txt
git checkout <commit_hash> <relative_path_to_file_or_dir>
git checkout <remote_name>/<branch_name> <file_or_dir>


git details
  add           add files
    git add -u                  // stage modified and deleted files
  branch        list, create, delete branches
    git branch --delete         // deletes, unless it has unmerged changes
      git branch --delete <branchname>
    git branch --delete --force // delete even if it has unmerged changes
      git branch --delete --force <branchname>
    git branch --list
    git branch <branchname>     // create new branch, does not checkout
      git checkout <branchname>
    git branch -m <oldname> <newname>   // rename branch
    git branch -m <newname>     // rename current branch
  checkout      switch branches or restore working trees
    git checkout -- <filen>     // clear modified copy of file
    git checkout <branchname>   // switch branch, used with git branch
      git branch --list         // returns list of branches
      git checkout <branchname> // switch to one of those branches
      git checkout -b <branchname>  // create and switch to branchname
      git checkout -b <newbranch> <existingbranch>
        // create newbranch from existingbranch and switch
      git checkout <remotebranch>
        git fetch --all         // fetch remote branch first
        git checkout <remotebranch>
  cherry-pick
    git cherry-pick hash
    git checkout -b newbranch               // checkout newbranch from parent branch
    git checkout -b newbranch hashid        // checkout newbranch with hashid as last commit from parent branch
    git rebase --onto master hashid
    git revert hashid                       // brings up edit page to say a hash id was reverted
    git revert reverted_hashid              // this reverts a reverted hashid. do this or merge or cherry pick
  clean         remove files from directory without saving
    git clean -n                // dry run of what gets removed
    git clean -f                // force
    git clean -f <filename>
    git clean -d <directory>
    git clean -df <directory>
    git clean -di               // interactive
  clone         clone repository
  commit        record changes to repository
    git commit --amend -m "message"
    git commit --amend --no-edit  // amend without changing commit message
    git commit -m "message"
  diff          diff              // git diff is inverse of git apply
    git diff HEAD ./path/filename
    git diff                      // unstaged changes
    git diff --cached ./path/filename
    git diff --cached             // diff staged changes
    git diff > patchfile.patch
      git diff --cached > patchfile
      git diff > patchfile
      git apply patchfile
      git apply --stat patchfile  // stat of apply, preview
      git apply --check patchfile // dry run
      git diff HEAD > patchfile   // create patch with new files and modified
  fetch
  init          choose empty repository, new repository
    git init
  log           show commit logs
    git log --oneline             // condensed view
    git log -n 3                  // show last 3 commits
    git show <commitid>           // show the details of commit
    git log --author="ngwayne|someoneelse"
    git log --author="ngwayne"
    git log --grep="AAX-1234"     // grep logs for AAX-1234
    git log -- filename1 filename2  // git log of filename
    git log --name-only           // git of filenames but not details
    git log -n 3 --name-only      // filenames of last 3 commits
    git log -n 3 -p               // show patches of last 3
    git log -- filename           // show log of filename for past 3
    git log -n 3 -- filename      // show log of filename for past 3,
                                  // -n 3 must be before
    git log --follow -p -- <file> // same as file log, but follows if renamed
  merge         join two or more histories
  mv            move file, directory
  pull          fetch and integrate with another repo or local branch
  push          update remote refs
  rebase        reapply commits on top of another base tip
    git rebase master
    git rebase master topic
    git rebase --continue
    git rebase --skip
    git rebase --abort
  reset
    --soft
    --mixed
    --hard
  revert
  rm
  show          show various objects
    git show <commitid>         // use git log to get commitid
    git show HEAD:/path/file    // show HEAD version of file
    git show <commit>:/path/file  // show version of file
  stash
    git stash list              // show stashes
    git stash show <stash>      // show files of a stash
      git stash show -p         // show full diff of a stash
      git stash show -p stash@{1} | git apply -R
        // unapplying stash
      git stash show -p | git apply -R
        // unapplying stash
    git stash drop <stash>
      git stash drop stash@{1}  // stash delete stash
    git stash pop <stash>
      git stash pop             // pop top of stash
      git stash pop stash@{2}   // pop stash @{2}
    git stash apply <stash>
      git stash apply stash@{1}
    git stash push --patch      // default stashes modified and tracked
                                // not include untracked and ignored
                                // to add new files, do git add <filename>
                                // then call git stash
    git stash                   // push changes to top of stash, tracked only
    git stash push              // push changes to top of stash
    git stash --keep-index      // stash, but also leave in index
    git stash -u                // stash untracked files and tracked files
    git stash --all             // stash all, including ignored
    git stash save "message"    // annotate the stash
    git stash -p                // interactive patch stash, iterates each
                                // file and prompts if stash
    git diff stash@{0}          // diff against stash
    git diff -p stash@{0} -p stash@{1}  // diff against 2 stashes
    git stash branch <newbranch> stash@{3}
                                // create branch from a stash
    git stash create <message>
    git diff [--options] <commit> <commit> [--] [<path>...]

    git diff <revision_1>:<file_1> <revision_2>:<file_2>
    For instance, to see the difference for a file "main.c" between now and two commits back, here are three equivalent commands:

    git diff HEAD^^ HEAD main.c
    git diff HEAD^^..HEAD -- main.c
    git diff HEAD~2 HEAD -- main.c


  status        working tree status


--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
jmx       -Dcom.sun.management.jmxremote
jcmd      jcmd ManagementAgent.start jdp.port=<port>
attach
jinfo
jstatd
java flight recorder
jvisualvm
jmap
jstack
jps
jstat
jmc       java mission control
jmap

jcmd -> attach -> jvm attach <-> dcmd <-> monitored jvm <-> jmx client
                  jvmstat     -> jstat -> jstat client rmi

--------------------------------------------------------------------------------
* toc_sql

- sqlite3
.tables
.schema
.mode columns
.header
.headers on
.headers off
.read filename
.output test.csv
.databases


--------------------------------------------------------------------------------

* toc_redis
- toc_redis_iterate_keys
- toc_redis_links

- toc_redis_iterate_keys
{
  keys is O(n)
  keys *
  keys prefix*
  keys *pattern*

  cursor based iterator scan
  scan | sscan | hscan | zscan

  a cursor (an integer) is returned when iteration is
    1. initialized
    2. updated value on every iteration
    3. 0 when cursor terminates
  full iteration always returns all elements from before start of iteration
  a given element can be returned multiple times, and app needs to handle dedupe

  count option used to limit the number of elements returned each iteration.
  default is 10.

  match option allows patterns, like keys.

  scan 0 count 5 match *pat*
    this returns tuple of (cursor, list of 5 values if present).
    to do the next iteration, do scan <cursor_val_unless_0>
    and this returns the next cursor, values tuple
  scan <cursor> count 10
    stop when cursor == 0
  scan 0
    this returns all keys in groups of 10 (default is 10)
  scan 0 count 20
  scan 0 count 20 match pattern*

  scan 0 count 1000
    means scan from 0 up to 1000
  scan 0 count 1000 match pat*
    means scan from 0 up to 1000 and return any matches, which can be <= 1000
    it does not mean scan all and return the first 1000 matches
    this means it can return empty
  use cursor 0 to start from beginning

  https://redis.io/commands/scan

  select db
  there are default 16 databases. use select <id> to select among them.
  flushdb     delete or flush all keys from current db


}

delete node
redis-trib.rb del-node ip:port nodeid

> memory stats
> memory usage   key [samples count]  // reports number of bytes the keyval uses
> memory doctor
>


operational:
  bgsave
  client list                         get list of connected client
  cluster addslots slot
  cluster delslots slot
  cluster failover force
  cluster meet ip port                for node cluster to handshake with another node
  cluster reset hard|sort             reset a node
  cluster slots
  config rewrite
  dbsize
  memory doctor
  memory purge
  quit
  replicaof host port                 make server repilica of another
  scan cursor match
  auth password
  cluster info
  cluster nodes
  cluster slaves nodeid               list replicas of master nodeid
  command info
  config set param val
  dump key
  memory stats
  slowlog
  sync
  sscan key cursor match              iterate set elements
  cluster forget nodeid               remove node from nodes table
  cluster keyslot
  cluster replicate nodeid
  cluster replicas nodeid
  config get
  slaveof host port
  hscan
data:
  exists key
  flushall async                      removes all keys from all databases
  hgetall
  hdel
  hkeys
  hmset
  keys
  mget key
{
redis slowlog
> slowlog get last_n_entries
  1)  1) (integer) a      unique progressive id
      2) (integer) b      timestamp
      3) (integer) c      microseconds time needed
      4) 1) "ping"        array composing args of command
  2)  1) (integer) d
      2) (integer) e
      3) (integer) f
      4) 1) "slowlog"     array composing args of command, eg slowlog get 100
         2) "get"
         3) "100"
}



redis-cli --cluster reshard ip:port
redis-cli -p 7000 cluster nodes
redis-cli --cluster check ip:port
redis-cli -h <ip> -p <port> set <key> <val>
redis-cli>
          cluster failover force                  // must be done on slave node
redis-cli --cluster failover force                // does this work?
redis-cli --cluster add-node ip:port ip:port
redis-cli --cluster add-node ip:port ip_existing:port_existing --cluster-slave
                                                  // does not specify master, just as A slave
redis-cli --cluster add-node ip:port ip_existing:port_existing --cluster-slave --cluster-master-id nodeid
redis-cli --cluster del-node ip:port nodeid
redis-cli --cluster check                         // check
redis-cli --cluster fix                           // fix how?
redis-cli --cluster import                        // live move to new cluster

mkdir cluster-test
cd cluster-test
mkdir 7000 7001 7002 7003 7004 7005
create redis.conf in each dir
start every instance:
  cd 7000
  ../redis-server ./redis.conf
redis-cli --cluster create ip:port ip:port ip:port ... --cluster-replicas 1

redis-py-cluster
redis-rb-cluster
predis

> info server       // version



- toc_redis_links


--------------------------------------------------------------------------------
